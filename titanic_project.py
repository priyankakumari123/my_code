# -*- coding: utf-8 -*-
"""Titanic _Project_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PTrVhHid2fnRng8XMq_jjX8Jne2NEod1
"""

from keras.models import Sequential
from keras.layers import Dense
from keras.models import Sequential, load_model
from keras.layers import Dense, Dropout, BatchNormalization, Activation
from keras.wrappers.scikit_learn import KerasRegressor
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
# %matplotlib inline
import seaborn as sns
import scipy.stats as stats
from numpy.random import seed
seed(10)
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve, auc
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xgb
from xgboost.sklearn import XGBClassifier

print(__doc__)

train= pd.read_csv("train.csv")
test= pd.read_csv("test.csv")

train.head()

print(train.shape)

print(train.dtypes)

train.info()

train.count()

train.isnull().sum()

train[['PassengerId','Pclass','Age','SibSp','Parch','Fare']].describe()

def bargraph(feature):
    survived= train[train['Survived']==1][feature].value_counts()
    dead= train[train['Survived']==0][feature].value_counts()
    df= pd.DataFrame([survived,dead])
    df.index= ('survived','dead')
    df.plot(kind='bar',stacked=True)

train['Survived'].value_counts()

train['Survived'].value_counts().plot(kind='bar')

train['Sex'].value_counts()

train['Sex'].value_counts().plot(kind='bar')

bargraph('Sex')

train['Survived'].value_counts()*100/len(train)



train['Pclass'].value_counts()

train['Pclass'].value_counts().plot(kind='bar')

bargraph('Pclass')

train['Embarked'].value_counts().plot(kind='bar')

bargraph('Embarked')

bargraph('SibSp')

titanic = [train, test] # combining train and test dataset

for dataset in titanic:
    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)

train['Title'].value_counts()

test['Title'].value_counts()

title_mapping = {"Mr": 0, "Miss": 1, "Mrs": 2, 
                 "Master": 3, "Dr": 4, "Rev": 4, "Col": 4, "Major": 4, "Mlle": 4,"Countess": 4,
                 "Ms": 4, "Lady": 4, "Jonkheer": 4, "Don": 4, "Dona" : 4, "Mme": 4,"Capt": 4,"Sir": 4 }
for dataset in titanic:
    dataset['Title'] = dataset['Title'].map(title_mapping)

train.head()

bargraph('Title')

train.replace(['male','female'],[0,1],inplace=True)
test.replace(['male','female'],[0,1],inplace=True)

# filling missing age with median age for each title 
train["Age"].fillna(train.groupby("Title")["Age"].transform("median"), inplace=True)
test["Age"].fillna(test.groupby("Title")["Age"].transform("median"), inplace=True)

#making Age to categorical variable
for dataset in titanic:
    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0,
    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1,
    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2,
    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3,
    dataset.loc[ dataset['Age'] > 62, 'Age'] = 4
train.head()

for dataset in titanic:
    dataset['family_size']= dataset['SibSp']+dataset['Parch']+1
train.head()

train['Embarked'].value_counts()
#train['Embarked'].fillna(train['Embarked'].mode())

for dataset in titanic:
    dataset['Embarked']= dataset['Embarked'].fillna('S')

for dataset in titanic:
    dataset['Embarked'] = dataset['Embarked'].replace(['S','C','Q'],[0,1,2])

train.isnull().sum()
train['Fare'].describe()

# making Fare numerical to categorical 
for dataset in titanic:
    dataset.loc[ dataset['Fare'] <= 10, 'Fare'] = 0,
    dataset.loc[ dataset['Fare'] > 10 & (dataset['Fare'] <= 30), 'Fare'] = 1
    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2,
    dataset.loc[ dataset['Fare'] > 100, 'Fare'] = 3

test.Fare.fillna(value= 1,inplace= True)

train.Cabin.value_counts()

for dataset in titanic:
    dataset['Cabin'] = dataset['Cabin'].str[:1]

train.Cabin.value_counts()

train = train.drop(['PassengerId','Name','SibSp', 'Parch','Ticket','Cabin'], axis=1)
test = test.drop(['PassengerId','Name','SibSp', 'Parch','Ticket','Cabin'], axis=1)

X = train.drop('Survived', axis=1)
#target = train['Survived']

X.shape

y= train['Survived']

from sklearn.preprocessing import StandardScaler
X = StandardScaler().fit_transform(X)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)

X_test.shape

test.shape

# KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier

tuned_parameters_knn = [{'n_neighbors': [5,10], 'weights': ['uniform','distance'],
                     'algorithm': ['brute','auto'], 'p': [1],'metric':['euclidean','minkowski']}
                    ]
scores = ['accuracy']
for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()

clf = GridSearchCV(KNeighborsClassifier(), tuned_parameters_knn, cv=10, scoring='%s' % score)
clf.fit(X_train, y_train)

print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()

means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()

y_true, y_pred = y_test, clf.predict(X_test)
print(classification_report(y_true, y_pred))
print("Detailed confusion matrix:")
print(confusion_matrix(y_true, y_pred))
print("Accuracy Score: \n")
print(accuracy_score(y_true, y_pred))
 
# Determine the false positive and true positive rates
FPR, TPR, _ = roc_curve(y_test, clf.predict_proba(X_test)[:,1])
 
# Calculate the AUC

roc_auc = auc(FPR, TPR)
print ('ROC AUC: %0.3f' % roc_auc )
 
# Plot of a ROC curve
plt.figure(figsize=(5,5))
plt.plot(FPR, TPR, label='ROC curve (area = %0.3f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()
print()

# GaussianNB classifier
from sklearn.naive_bayes import GaussianNB

tuned_parameters_GNB = [{'priors':[(0.7,0.3),(0.4,0.6),(0.9,0.1),(0.2,0.8)]}]
                    
scores = ['accuracy']
for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()
    
clf = GridSearchCV(GaussianNB(), tuned_parameters_GNB, cv=10, scoring='%s' % score)
clf.fit(X_train, y_train)

print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()

means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()
y_true, y_pred = y_test, clf.predict(X_test)
print(classification_report(y_true, y_pred))
print("Detailed confusion matrix:")
print(confusion_matrix(y_true, y_pred))
print("Accuracy Score: \n")
print(accuracy_score(y_true, y_pred))
 
# Determine the false positive and true positive rates
FPR, TPR, _ = roc_curve(y_test, clf.predict_proba(X_test)[:,1])
 
# Calculate the AUC

roc_auc = auc(FPR, TPR)
print ('ROC AUC: %0.3f' % roc_auc )
 
# Plot of a ROC curve
plt.figure(figsize=(5,5))
plt.plot(FPR, TPR, label='ROC curve (area = %0.3f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()
print()

# LogisticRegression classifier
from sklearn.linear_model import LogisticRegression

tuned_parameters_LR = [{'C': [ 10, 100,1000], 'max_iter': [100,150], 'multi_class':['ovr'], 'tol':[1e-4,1e-6],
                         'random_state':[19]}]
scores = ['accuracy']
for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()
    
clf = GridSearchCV(LogisticRegression(), tuned_parameters_LR, cv=10, scoring='%s' % score)
clf.fit(X_train, y_train)
print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()

means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()

y_true, y_pred = y_test, clf.predict(X_test)
print(classification_report(y_true, y_pred))
print("Detailed confusion matrix:")
print(confusion_matrix(y_true, y_pred))
print("Accuracy Score: \n")
print(accuracy_score(y_true, y_pred))
 
# false positive and true positive rates
FPR, TPR, _ = roc_curve(y_test, clf.predict_proba(X_test)[:,1])
 
# AUC calculation

roc_auc = auc(FPR, TPR)
print ('ROC AUC: %0.3f' % roc_auc )
 
# ROC curve
plt.figure(figsize=(5,5))
plt.plot(FPR, TPR, label='ROC curve (area = %0.3f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()
print()

#DecisionTreeClassifier
# scores = ['accuracy']
#param_grid = {'max_depth': np.arange(1, 11)}
param_grid = {'max_depth':[1,2,3,5,10,],'min_samples_split':[2,3,4,5,10],'max_leaf_nodes':[2,3,5,10,15],
               'min_samples_leaf': [2,5,10,15],'random_state':[11]}
print(param_grid)
for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()

    tree = GridSearchCV(DecisionTreeClassifier(), param_grid,scoring='accuracy')
    tree.fit(X_train,y_train)
    print("Best parameters ")
    print()
    print(tree.best_params_)
    print()
    print("Grid scores on development set:")
    print()
    means = tree.cv_results_['mean_test_score']
    stds = tree.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds, tree.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r"
#               % (mean, std * 2, params))
    print()

    print("classification report:")
    print()
    print("The model is trained on the full development set.")
    print("The scores are computed on the full evaluation set.")
    print()
    y_true, y_pred = y_test, tree.predict(X_test)
    print(classification_report(y_true, y_pred))
    print("confusion matrix:")
    print(confusion_matrix(y_true, y_pred))
    print("Accuracy Score: \n")
    print(accuracy_score(y_true, y_pred))
    # false positive and true positive rates
    FPR, TPR, _ = roc_curve(y_test, clf.predict_proba(X_test)[:,1])

    # AUC calculation

    roc_auc = auc(FPR, TPR)
    print ('ROC AUC: %0.3f' % roc_auc )

    # ROC curve
    plt.figure(figsize=(5,5))
    plt.plot(FPR, TPR, label='ROC curve (area = %0.3f)' % roc_auc)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([-0.05, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.show()
    print()

# SVM classifier
tuned_parameters = [{'kernel': ['rbf', 'linear'], 'gamma': [0.01,0.1,1,2],
                     'C': [ 0.1, 1, 10,100], 'max_iter':[1000],'random_state':[10]}
                    ]
# We are going to limit ourselves to accuracy score, other options can be
# seen here:
# http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter
# Some other values used are the predcision_macro, recall_macro
scores = ['accuracy']

for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()

    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,
                       scoring='%s' % score)
    clf.fit(X_train, y_train)

    print("Best parameters set found on development set:")
    print()
    print(clf.best_params_)
    print()
    print("Grid scores on development set:")
    print()
    means = clf.cv_results_['mean_test_score']
    stds = clf.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds, clf.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r"
#               % (mean, std * 2, params))
    print()

    print("Detailed classification report:")
    print()
    print("The model is trained on the full development set.")
    print("The scores are computed on the full evaluation set.")
    print()
    y_true, y_pred = y_test, clf.predict(X_test)
    print(classification_report(y_true, y_pred))
    print("Detailed confusion matrix:")
    print(confusion_matrix(y_true, y_pred))
    print("Accuracy Score: \n")
    print(accuracy_score(y_true, y_pred))
    print()

# MLPClassifier
from sklearn.neural_network import MLPClassifier

scores = ['accuracy']
parameters={'learning_rate': ["constant", "invscaling", "adaptive"],'hidden_layer_sizes': [10,20,10,15],'alpha': [0.001,0.01,0.1,1],'activation': ["identity","logistic", "relu", "tanh"], 'random_state':[11]}
for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()

    n_net = GridSearchCV(MLPClassifier(), parameters,scoring='accuracy')
    n_net.fit(X_train,y_train)
    print("Best parameters set found on development set:")
    print()
    print(n_net.best_params_)
    print()
    print("Grid scores on development set:")
    print()
    means = n_net.cv_results_['mean_test_score']
    stds = n_net.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds, n_net.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r"
#               % (mean, std * 2, params))
    print()

    print("Detailed classification report:")
    print()
    print("The model is trained on the full development set.")
    print("The scores are computed on the full evaluation set.")
    print()
    y_true, y_pred = y_test, n_net.predict(X_test)
    print(classification_report(y_true, y_pred))
    print("Detailed confusion matrix:")
    print(confusion_matrix(y_true, y_pred))
    print("Accuracy Score: \n")
    print(accuracy_score(y_true, y_pred))

# Titanic test datahas been evaluated on MLP classifier 
#as It is maximum accuracy of 86.6%
Testing= n_net.predict(test)
print(Testing)

#Random_forest classifier
from sklearn.ensemble import RandomForestClassifier

scores = ['accuracy']
parameters = {'n_estimators': [50,70,100], 
              'max_features': ['log2', 'sqrt','auto'], 
              'criterion': ['entropy', 'gini'],
              'max_depth': [ 5, 10,20], 
              'min_samples_split': [2, 3, 5],
              'min_samples_leaf': [1,5,10]}
for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()

    Rand_Forest = GridSearchCV(RandomForestClassifier(), parameters,scoring='accuracy')
    Rand_Forest.fit(X_train,y_train)
    print("Best parameters set found on development set:")
    print()
    print(Rand_Forest.best_params_)
    print()
    print("Grid scores on development set:")
    print()
    means = Rand_Forest.cv_results_['mean_test_score']
    stds =  Rand_Forest.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds, Rand_Forest.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r"
#               % (mean, std * 2, params))
    print()

    print("Detailed classification report:")
    print()
    print("The model is trained on the full development set.")
    print("The scores are computed on the full evaluation set.")
    print()
    y_true, y_pred = y_test,Rand_Forest.predict(X_test)
    print(classification_report(y_true, y_pred))
    print("Detailed confusion matrix:")
    print(confusion_matrix(y_true, y_pred))
    print("Accuracy Score: \n")
    print(accuracy_score(y_true, y_pred))
    # false positive and true positive rates
    FPR, TPR, _ = roc_curve(y_test, Rand_Forest.predict_proba(X_test)[:,1])

    # AUC calculation

    roc_auc = auc(FPR, TPR)
    print ('ROC AUC: %0.3f' % roc_auc )

    # ROC curve
    plt.figure(figsize=(5,5))
    plt.plot(FPR, TPR, label='ROC curve (area = %0.3f)' % roc_auc)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([-0.05, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.show()
    print()

# Gradient Boosting Classifier

from sklearn.ensemble import GradientBoostingClassifier

tuned_parameters_GBC = [{'n_estimators': [1000], 'learning_rate': [1,2,5],
                     'max_features': [4], 'max_depth' : [3,4,5]}
                    ]
scores = ['accuracy']
for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()

clf = GridSearchCV(GradientBoostingClassifier(), tuned_parameters_GBC, cv=5, scoring='%s' % score)
clf.fit(X_train, y_train)

print("Best parameters set found on development set:")
print()
print(clf.best_params_)
print()
print("Grid scores on development set:")
print()

means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))
print()

print("Detailed classification report:")
print()
print("The model is trained on the full development set.")
print("The scores are computed on the full evaluation set.")
print()

y_true, y_pred = y_test, clf.predict(X_test)
print(classification_report(y_true, y_pred))
print("Detailed confusion matrix:")
print(confusion_matrix(y_true, y_pred))
print("Accuracy Score: \n")
print(accuracy_score(y_true, y_pred))
# false positive and true positive rates
FPR, TPR, _ = roc_curve(y_test, clf.predict_proba(X_test)[:,1])
 
# AUC calculation

roc_auc = auc(FPR, TPR)
print ('ROC AUC: %0.3f' % roc_auc )
 
# ROC curve
plt.figure(figsize=(5,5))
plt.plot(FPR, TPR, label='ROC curve (area = %0.3f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()
print()

#Neural_Net
# hidden layers
model.add(Dense(8))
model.add(Activation("sigmoid"))
model.add(Dropout(0.2))

model.add(Dense(4))
model.add(Activation("sigmoid"))
model.add(Dropout(0.2))
    

    
model.add(Dense(2, activation="sigmoid"))
    
# output layer
model.add(Dense(1, activation='sigmoid'))

# model compile for binary classification
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

Ann= model.fit(X_train, y_train, validation_data=(X_test,y_test),nb_epoch=1000)

plt.plot(Ann.history['loss'])
plt.plot(Ann.history['val_loss'])
plt.title('Model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for accuracy
plt.plot(Ann.history['acc'])
plt.plot(Ann.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Xgboost classifier
scores = ['accuracy']

params = {'learning_rate':[0.01,0.05],'n_estimators':[100,150,200],'max_delta_step':[1,4,7],'seed':[1234,70030]}
                  
for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()
    xgb = xgb.XGBClassifier()
    XGB = GridSearchCV(xgb, param_grid=params, scoring='accuracy', cv=10 )
    XGB.fit(X_train,y_train)
    print("Best parameters set found on development set:")
    print()
    print(XGB.best_params_)
    print()
    print("Grid scores on development set:")
    print()
    means = XGB.cv_results_['mean_test_score']
    stds =  XGB.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds,  XGB.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r"
#               % (mean, std * 2, params))
    print()

    print("Detailed classification report:")
    print()
    print("The model is trained on the full development set.")
    print("The scores are computed on the full evaluation set.")
    print()
    y_true, y_pred = y_test, XGB.predict(X_test)
    print(classification_report(y_true, y_pred))
    print("Detailed confusion matrix:")
    print(confusion_matrix(y_true, y_pred))
    print("Accuracy Score: \n")
    print(accuracy_score(y_true, y_pred))

    print()